# general options
checkpoints_dir: checkpoints
tokenizer_basename: tokenizer.json
project_name: pre-training-gpt2
expr_name: wala
wandb_resume_id: null
seed: 1061109567

# dataset
data_file_path: ./datasets/example-data.txt
data_save_path: ./datasets
test_size: 0.1

# model
vocab_size: 32_000
seq_length: 128
d_model: 768
num_layers: 12
num_heads: 12
d_ff: 3072
dropout: 0.1
activation: gelu

# training
optim: adamw # possible values are: adam, adamw
weight_decay: 1.0e-4
lr: 0.5
warmup_steps: 4_000
batch_size: 32
gradient_accum_step: 1
fp16: false
train_steps: 40_000
valid_steps: 2_000
valid_interval: 3_000
save_interval: 4_000
max_grad_norm: 1.0
preload_checkpoint: null
